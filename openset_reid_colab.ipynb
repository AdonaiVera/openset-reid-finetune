{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üß† SigLIP Person Finder - Complete Google Colab Setup\n",
        "\n",
        "This notebook provides a complete setup for the open-set person search system using natural language descriptions and a fine-tuned SigLIP model.\n",
        "\n",
        "## What this notebook does:\n",
        "1. ‚úÖ Clone the repository\n",
        "2. ‚úÖ Install all dependencies\n",
        "3. ‚úÖ Download the dataset\n",
        "4. ‚úÖ Train the SigLIP model\n",
        "5. ‚úÖ Run inference on images\n",
        "6. ‚úÖ Visualize results\n",
        "\n",
        "## Features:\n",
        "- Text-based person search using natural language descriptions\n",
        "- Multi-view ReID dataset with rich semantic attributes\n",
        "- Real-time person detection with YOLOv8\n",
        "- Fine-tuned SigLIP model for open-set retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üöÄ Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "clone_repo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e056dc53-5da9-458f-aeb3-30c6db0598bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'openset-reid-finetune'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 78 (delta 32), reused 51 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (78/78), 1.50 MiB | 29.49 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n",
            "/content/openset-reid-finetune/openset-reid-finetune\n",
            "‚úÖ Repository cloned successfully!\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/AdonaiVera/openset-reid-finetune\n",
        "%cd openset-reid-finetune\n",
        "print(\"‚úÖ Repository cloned successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "install_dependencies",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f45ffad-dece-4b5f-c664-41d94b2b4cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All dependencies installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install all required dependencies\n",
        "!pip install -q datasets>=3.6.0 fiftyone>=1.5.2 google-generativeai>=0.8.5 \\\n",
        "    gradio>=5.33.0 huggingface-hub>=0.32.3 numpy>=2.2.6 pillow>=11.2.1 \\\n",
        "    python-dotenv>=1.1.0 sentencepiece>=0.2.0 spaces>=0.36.0 torch>=2.7.0 \\\n",
        "    torchvision>=0.22.0 transformers>=4.52.4 ultralytics>=8.3.148 wandb>=0.19.11 \\\n",
        "    tqdm opencv-python\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "check_gpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896056ae-f1b8-4a29-defc-8421077636dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected. Training will be slower on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dataset_section"
      },
      "source": [
        "## üìä Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NJzFWGxbsPEk"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your Hugging Face token here\n",
        "login(\"your-huggingface-token\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "download_dataset",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63742789-8bc6-4578-ad1e-2fae8af76a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Downloading dataset from Hugging Face...\n",
            "Downloading config file fiftyone.yml from adonaivera/fiftyone-multiview-reid-attributes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.huggingface:Downloading config file fiftyone.yml from adonaivera/fiftyone-multiview-reid-attributes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.huggingface:Loading dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.data.importers:Importing samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6455/6455 [668.7ms elapsed, 0s remaining, 9.8K samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6455/6455 [668.7ms elapsed, 0s remaining, 9.8K samples/s]       \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded successfully!\n",
            "üìä Total samples: 6455\n",
            "üè∑Ô∏è Tags: {'train': 3181, 'query': 3269, 'gallery': 5}\n",
            "\n",
            "üì∏ Sample image: /root/fiftyone/huggingface/hub/adonaivera/fiftyone-multiview-reid-attributes/data/00000002_01.jpg\n",
            "üë§ Person ID: 2\n",
            "üìù Description: The person is a male adult wearing a red t-shirt, blue denim shorts, and dark sandals. He has short, dark hair and is walking.\n",
            "üè∑Ô∏è Attributes: {'gender': 'Male', 'age': 'Adult', 'ethnicity': 'Unknown', 'occupation': 'Unknown', 'appearance': {'hair': {'type': 'Short', 'color': 'Black', 'description': 'Short, dark hair.'}, 'beard': {'type': 'None', 'color': 'Unknown', 'description': 'No beard visible.'}, 'expression': {'type': 'Unknown', 'description': 'Facial expression is not clearly visible.'}}, 'posture': {'type': 'Walking', 'description': 'The person is walking.'}, 'actions': {'type': 'Walking', 'description': 'The person is walking.'}, 'clothing': {'upper': {'type': 'T-shirt', 'color': 'Red', 'description': 'A red t-shirt with a logo.'}, 'lower': {'type': 'Shorts', 'color': 'Blue', 'description': 'Blue denim shorts.'}, 'shoes': {'type': 'Sandals', 'color': 'Unknown', 'description': 'Dark-colored sandals.'}}, 'accessories': {'hat': {'type': 'None', 'color': 'Unknown', 'description': 'No hat is visible.'}, 'glasses': {'type': 'None', 'color': 'Unknown', 'description': 'No glasses are visible.'}}, 'description': 'The person is a male adult wearing a red t-shirt, blue denim shorts, and dark sandals. He has short, dark hair and is walking.'}\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset from Hugging Face\n",
        "import fiftyone as fo\n",
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "print(\"üì• Downloading dataset from Hugging Face...\")\n",
        "dataset = load_from_hub(\n",
        "    repo_id=\"adonaivera/fiftyone-multiview-reid-attributes\",\n",
        "    dataset_name=\"fiftyone-multiview-reid2\",\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"üìä Total samples: {len(dataset)}\")\n",
        "print(f\"üè∑Ô∏è Tags: {dataset.count_values('tags')}\")\n",
        "\n",
        "# Show a sample\n",
        "sample = dataset.first()\n",
        "print(f\"\\nüì∏ Sample image: {sample.filepath}\")\n",
        "print(f\"üë§ Person ID: {sample.person_id}\")\n",
        "print(f\"üìù Description: {sample.description}\")\n",
        "if sample.attributes:\n",
        "    print(f\"üè∑Ô∏è Attributes: {sample.attributes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_dataset"
      },
      "outputs": [],
      "source": [
        "# Visualize some samples from the dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def show_samples(dataset, num_samples=6):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    samples = dataset.limit(num_samples)\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        img = Image.open(sample.filepath)\n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f\"Person {sample.person_id}\\n{sample.description[:50]}...\")\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show training samples\n",
        "print(\"üì∏ Training samples:\")\n",
        "train_samples = dataset.match_tags(\"train\")\n",
        "show_samples(train_samples)\n",
        "\n",
        "# Show query samples\n",
        "print(\"üîç Query samples:\")\n",
        "query_samples = dataset.match_tags(\"query\")\n",
        "show_samples(query_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_section"
      },
      "source": [
        "## üß† Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_training"
      },
      "outputs": [],
      "source": [
        "# Import training modules\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from utils.datasets import TextImageDataset, load_dataset\n",
        "from utils.collators import TextImageCollator\n",
        "from transformers import AutoProcessor, SiglipModel, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ Training modules imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_config"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "class TrainingConfig:\n",
        "    def __init__(self):\n",
        "        self.epochs = 5  # Reduced for Colab demo\n",
        "        self.batch_size = 8  # Reduced for Colab memory\n",
        "        self.lr = 1e-5\n",
        "        self.save_dir = \"models\"\n",
        "        self.patience = 3\n",
        "        self.temperature = 0.07\n",
        "\n",
        "config = TrainingConfig()\n",
        "print(f\"‚öôÔ∏è Training config:\")\n",
        "print(f\"   Epochs: {config.epochs}\")\n",
        "print(f\"   Batch size: {config.batch_size}\")\n",
        "print(f\"   Learning rate: {config.lr}\")\n",
        "print(f\"   Save directory: {config.save_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_dataloaders"
      },
      "outputs": [],
      "source": [
        "# Prepare dataloaders\n",
        "print(\"üìä Preparing dataloaders...\")\n",
        "\n",
        "# Load model and processor\n",
        "model = SiglipModel.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Create datasets\n",
        "train_ds = TextImageDataset(dataset, split=\"train\", processor=processor, augment=True)\n",
        "val_ds = TextImageDataset(dataset, split=\"query\", processor=processor, augment=False)\n",
        "\n",
        "# Create dataloaders\n",
        "train_dl = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=TextImageCollator(processor),\n",
        "    num_workers=2,  # Reduced for Colab\n",
        ")\n",
        "\n",
        "val_dl = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=TextImageCollator(processor),\n",
        "    num_workers=2,  # Reduced for Colab\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataloaders created!\")\n",
        "print(f\"   Training batches: {len(train_dl)}\")\n",
        "print(f\"   Validation batches: {len(val_dl)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_loop"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "print(\"üöÄ Starting training...\")\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs(config.save_dir, exist_ok=True)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=0.01)\n",
        "num_training_steps = config.epochs * len(train_dl)\n",
        "scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=50, num_training_steps=num_training_steps)\n",
        "\n",
        "# Training history\n",
        "train_losses = []\n",
        "val_scores = []\n",
        "best_loss = float(\"inf\")\n",
        "no_improve_epochs = 0\n",
        "\n",
        "for epoch in range(config.epochs):\n",
        "    print(f\"\\nüìà Epoch {epoch + 1}/{config.epochs}\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_dl, desc=\"Training\")):\n",
        "        batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
        "\n",
        "        image_features = model.get_image_features(pixel_values=batch['pixel_values'])\n",
        "        text_features = model.get_text_features(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "\n",
        "        image_features = F.normalize(image_features, dim=-1)\n",
        "        text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "        logits_per_image = torch.matmul(image_features, text_features.T) / config.temperature\n",
        "        labels = torch.arange(len(image_features), device=device)\n",
        "\n",
        "        loss = (F.cross_entropy(logits_per_image, labels) + F.cross_entropy(logits_per_image.T, labels)) / 2\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(f\"   Step {step} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_dl)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    all_image_feats = []\n",
        "    all_text_feats = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_dl, desc=\"Validation\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items() if torch.is_tensor(v)}\n",
        "\n",
        "            image_feats = model.get_image_features(batch['pixel_values'])\n",
        "            text_feats = model.get_text_features(batch['input_ids'], batch['attention_mask'])\n",
        "\n",
        "            image_feats = F.normalize(image_feats, dim=-1)\n",
        "            text_feats = F.normalize(text_feats, dim=-1)\n",
        "\n",
        "            all_image_feats.append(image_feats)\n",
        "            all_text_feats.append(text_feats)\n",
        "\n",
        "        image_feats = torch.cat(all_image_feats, dim=0)\n",
        "        text_feats = torch.cat(all_text_feats, dim=0)\n",
        "\n",
        "        similarity_matrix = image_feats @ text_feats.T\n",
        "        target = torch.arange(similarity_matrix.size(0)).to(device)\n",
        "\n",
        "        # Image-to-text retrieval\n",
        "        top1 = similarity_matrix.topk(1, dim=1).indices.squeeze()\n",
        "        recall_at_1 = (top1 == target).float().mean().item()\n",
        "\n",
        "        top5 = similarity_matrix.topk(5, dim=1).indices\n",
        "        recall_at_5 = (top5 == target.unsqueeze(1)).any(dim=1).float().mean().item()\n",
        "\n",
        "        val_score = recall_at_5\n",
        "        val_scores.append(val_score)\n",
        "\n",
        "    print(f\"üìä Epoch {epoch + 1} | Train Loss: {avg_loss:.4f} | Val Recall@1: {recall_at_1:.4f} | Val Recall@5: {recall_at_5:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        no_improve_epochs = 0\n",
        "        best_path = os.path.join(config.save_dir, f\"best_model_epoch_{epoch + 1}_loss_{avg_loss:.4f}.pt\")\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"‚úÖ New best model saved to {best_path}\")\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "        print(f\"‚ö†Ô∏è No improvement. Patience counter: {no_improve_epochs}/{config.patience}\")\n",
        "\n",
        "    if no_improve_epochs >= config.patience:\n",
        "        print(\"üõë Early stopping triggered.\")\n",
        "        break\n",
        "\n",
        "print(\"üéâ Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_training"
      },
      "outputs": [],
      "source": [
        "# Plot training progress\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot training loss\n",
        "ax1.plot(train_losses, 'b-', label='Training Loss')\n",
        "ax1.set_title('Training Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot validation score\n",
        "ax2.plot(val_scores, 'r-', label='Validation Recall@5')\n",
        "ax2.set_title('Validation Recall@5')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Recall@5')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üìà Final training loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"üìà Final validation recall@5: {val_scores[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inference_section"
      },
      "source": [
        "## üîç Inference and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_trained_model"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "print(\"üîß Loading trained model...\")\n",
        "\n",
        "# Find the best model file\n",
        "model_files = [f for f in os.listdir(config.save_dir) if f.endswith('.pt')]\n",
        "if model_files:\n",
        "    best_model_file = sorted(model_files)[-1]  # Get the latest model\n",
        "    model_path = os.path.join(config.save_dir, best_model_file)\n",
        "    print(f\"üìÅ Loading model from: {model_path}\")\n",
        "\n",
        "    # Load model weights\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Trained model loaded successfully!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No trained model found. Using pretrained model.\")\n",
        "    model = SiglipModel.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "    model.to(device)\n",
        "    model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_yolo"
      },
      "outputs": [],
      "source": [
        "# Download YOLOv8 model for person detection\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print(\"üì• Downloading YOLOv8 model...\")\n",
        "detector = YOLO(\"yolov8n.pt\")\n",
        "print(\"‚úÖ YOLOv8 model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_inference"
      },
      "outputs": [],
      "source": [
        "# Test inference on a sample from the dataset\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def test_inference_on_sample(sample, text_prompt):\n",
        "    \"\"\"Test inference on a single sample from the dataset\"\"\"\n",
        "\n",
        "    # Load image\n",
        "    image = cv2.imread(sample.filepath)\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detect persons with YOLO\n",
        "    results = detector(rgb_image)[0]\n",
        "    boxes = results.boxes\n",
        "\n",
        "    # Encode prompt text\n",
        "    with torch.no_grad():\n",
        "        text_inputs = processor(text=text_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
        "        text_feat = model.get_text_features(**text_inputs)\n",
        "        text_feat = torch.nn.functional.normalize(text_feat, dim=-1)\n",
        "\n",
        "    # Process detections\n",
        "    if boxes is not None:\n",
        "        for box in boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
        "            cls_id = int(box.cls[0].item())\n",
        "\n",
        "            if cls_id != 0:  # Only process persons\n",
        "                continue\n",
        "\n",
        "            # Draw default box (green)\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Crop and encode with SigLIP\n",
        "            crop = rgb_image[y1:y2, x1:x2]\n",
        "            image_input = processor(images=crop, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                image_feat = model.get_image_features(**image_input)\n",
        "                image_feat = torch.nn.functional.normalize(image_feat, dim=-1)\n",
        "                sim = torch.matmul(image_feat, text_feat.T).item()\n",
        "\n",
        "            # Label the similarity\n",
        "            cv2.putText(image, f\"{sim:.2f}\", (x1, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "            # Highlight matched person\n",
        "            if sim > 0.15:  # Threshold\n",
        "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                cv2.putText(image, \"MATCH!\", (x1, y1 - 30),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "\n",
        "    return image, sim if boxes is not None and len(boxes) > 0 else 0.0\n",
        "\n",
        "# Test on a few samples\n",
        "test_samples = query_samples.limit(3)\n",
        "\n",
        "for i, sample in enumerate(test_samples):\n",
        "    print(f\"\\nüîç Testing sample {i+1}:\")\n",
        "    print(f\"üìù Original description: {sample.description}\")\n",
        "\n",
        "    # Use the original description as the query\n",
        "    result_image, similarity = test_inference_on_sample(sample, sample.description)\n",
        "\n",
        "    print(f\"üìä Similarity score: {similarity:.4f}\")\n",
        "\n",
        "    # Display result\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Query: {sample.description[:50]}...\\nSimilarity: {similarity:.4f}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_inference"
      },
      "outputs": [],
      "source": [
        "# Custom inference with your own text prompts\n",
        "def custom_person_search(image_path, text_prompt, similarity_threshold=0.15):\n",
        "    \"\"\"Search for a person in an image using a text description\"\"\"\n",
        "\n",
        "    # Load image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"‚ùå Could not load image: {image_path}\")\n",
        "        return None, 0.0\n",
        "\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detect persons with YOLO\n",
        "    results = detector(rgb_image)[0]\n",
        "    boxes = results.boxes\n",
        "\n",
        "    # Encode prompt text\n",
        "    with torch.no_grad():\n",
        "        text_inputs = processor(text=text_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
        "        text_feat = model.get_text_features(**text_inputs)\n",
        "        text_feat = torch.nn.functional.normalize(text_feat, dim=-1)\n",
        "\n",
        "    max_similarity = 0.0\n",
        "\n",
        "    # Process detections\n",
        "    if boxes is not None:\n",
        "        for box in boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
        "            cls_id = int(box.cls[0].item())\n",
        "\n",
        "            if cls_id != 0:  # Only process persons\n",
        "                continue\n",
        "\n",
        "            # Draw default box (green)\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # Crop and encode with SigLIP\n",
        "            crop = rgb_image[y1:y2, x1:x2]\n",
        "            image_input = processor(images=crop, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                image_feat = model.get_image_features(**image_input)\n",
        "                image_feat = torch.nn.functional.normalize(image_feat, dim=-1)\n",
        "                sim = torch.matmul(image_feat, text_feat.T).item()\n",
        "\n",
        "            max_similarity = max(max_similarity, sim)\n",
        "\n",
        "            # Label the similarity\n",
        "            cv2.putText(image, f\"{sim:.2f}\", (x1, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "            # Highlight matched person\n",
        "            if sim > similarity_threshold:\n",
        "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "                cv2.putText(image, \"MATCH!\", (x1, y1 - 30),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
        "\n",
        "    return image, max_similarity\n",
        "\n",
        "# Test with a sample from the dataset\n",
        "sample = query_samples.first()\n",
        "print(f\"üîç Testing custom search on: {sample.filepath}\")\n",
        "print(f\"üìù Query: {sample.description}\")\n",
        "\n",
        "result_image, similarity = custom_person_search(sample.filepath, sample.description)\n",
        "\n",
        "if result_image is not None:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Query: {sample.description}\\nMax Similarity: {similarity:.4f}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"‚úÖ Search completed! Max similarity: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_section"
      },
      "source": [
        "## üì§ Upload Your Own Images (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_image"
      },
      "outputs": [],
      "source": [
        "# Upload your own image for testing\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"üì§ Upload an image to test the person search system...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded file\n",
        "uploaded_filename = list(uploaded.keys())[0]\n",
        "print(f\"‚úÖ Uploaded: {uploaded_filename}\")\n",
        "\n",
        "# Test with different text prompts\n",
        "test_prompts = [\n",
        "    \"A person wearing casual clothes\",\n",
        "    \"A man in a business suit\",\n",
        "    \"A woman with a handbag\",\n",
        "    \"Someone wearing jeans and a t-shirt\",\n",
        "    \"A person carrying a backpack\"\n",
        "]\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    print(f\"\\nüîç Testing prompt: {prompt}\")\n",
        "    result_image, similarity = custom_person_search(uploaded_filename, prompt)\n",
        "\n",
        "    if result_image is not None:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(f\"Query: {prompt}\\nSimilarity: {similarity:.4f}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"üìä Similarity score: {similarity:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## üéâ Summary\n",
        "\n",
        "Congratulations! You've successfully:\n",
        "\n",
        "‚úÖ **Cloned the repository** and set up the environment\n",
        "‚úÖ **Installed all dependencies** including PyTorch, Transformers, and YOLOv8\n",
        "‚úÖ **Downloaded the dataset** with rich semantic attributes\n",
        "‚úÖ **Trained a SigLIP model** for open-set person search\n",
        "‚úÖ **Performed inference** on images with text descriptions\n",
        "‚úÖ **Tested the system** with custom prompts and uploaded images\n",
        "\n",
        "## üîß Key Features Implemented:\n",
        "\n",
        "- **Text-based person search** using natural language descriptions\n",
        "- **Multi-view ReID dataset** with 6,455 samples and rich attributes\n",
        "- **YOLOv8 person detection** for automatic bounding box generation\n",
        "- **Fine-tuned SigLIP model** for improved text-image similarity\n",
        "- **Cosine similarity scoring** with configurable thresholds\n",
        "- **Real-time inference** with optimized tracking for videos\n",
        "\n",
        "## üöÄ Next Steps:\n",
        "\n",
        "1. **Experiment with different text prompts** to find the best descriptions\n",
        "2. **Adjust similarity thresholds** based on your use case\n",
        "3. **Try video inference** using the `inference_video.py` script\n",
        "4. **Fine-tune the model** on your own dataset for better performance\n",
        "5. **Deploy the model** for real-world applications\n",
        "\n",
        "## üìö Resources:\n",
        "\n",
        "- [Original Repository](https://github.com/AdonaiVera/openset-reid-finetune)\n",
        "- [Hugging Face Model](https://huggingface.co/adonaivera/siglip-person-search-openset)\n",
        "- [Dataset](https://huggingface.co/datasets/adonaivera/fiftyone-multiview-reid-attributes)\n",
        "- [Gradio Demo](https://huggingface.co/spaces/adonaivera/siglip-person-finder)\n",
        "\n",
        "Happy person searching! üïµÔ∏è‚Äç‚ôÄÔ∏è"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}